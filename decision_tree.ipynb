{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G91gGAaDETwT",
        "outputId": "82f5c458-b935-4141-f507-409e8593fab7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Information Gain for Attributes:\n",
            "Plant_ID: 0.8366407419411672\n",
            "Plant_Type: 0.8366407419411672\n",
            "Leaf_Color: 0.24911347578737209\n",
            "Leaf_Spots: 0.37686761179184985\n",
            "Leaf_Size: 0.06653280965494357\n",
            "Severity: 0.37686761179184985\n",
            "Attribute with Maximum Information Gain: Plant_ID\n",
            "Attribute with Minimum Information Gain: Leaf_Size\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/DATASET/plant.csv\")\n",
        "\n",
        "# Define the target variable (Infected) and the feature variables\n",
        "X = data.drop(columns=[\"Infected\"])\n",
        "y = data[\"Infected\"]\n",
        "\n",
        "# Define a function to calculate information gain\n",
        "def information_gain(feature, target):\n",
        "    # Calculate the entropy of the target variable\n",
        "    def entropy(y):\n",
        "        unique_classes = np.unique(y)\n",
        "        entropy = 0\n",
        "        for c in unique_classes:\n",
        "            p_c = np.sum(y == c) / len(y)\n",
        "            entropy -= p_c * np.log2(p_c)\n",
        "        return entropy\n",
        "\n",
        "    # Calculate the conditional entropy\n",
        "    def conditional_entropy(feature, target):\n",
        "        unique_values = np.unique(feature)\n",
        "        cond_entropy = 0\n",
        "        for value in unique_values:\n",
        "            subset_y = target[feature == value]\n",
        "            cond_entropy += (len(subset_y) / len(target)) * entropy(subset_y)\n",
        "        return cond_entropy\n",
        "\n",
        "    # Calculate information gain\n",
        "    return entropy(target) - conditional_entropy(feature, target)\n",
        "\n",
        "# Calculate information gain for all attributes\n",
        "information_gains = {}\n",
        "for column in X.columns:\n",
        "    information_gains[column] = information_gain(X[column], y)\n",
        "\n",
        "# Find the attribute with the maximum and minimum information gain\n",
        "max_info_gain_attribute = max(information_gains, key=information_gains.get)\n",
        "min_info_gain_attribute = min(information_gains, key=information_gains.get)\n",
        "\n",
        "# Print the information gain for all attributes, the attribute with maximum information gain, and the attribute with minimum information gain\n",
        "print(\"Information Gain for Attributes:\")\n",
        "for attribute, gain in information_gains.items():\n",
        "    print(f\"{attribute}: {gain}\")\n",
        "print(f\"Attribute with Maximum Information Gain: {max_info_gain_attribute}\")\n",
        "print(f\"Attribute with Minimum Information Gain: {min_info_gain_attribute}\")\n",
        "\n"
      ]
    }
  ]
}